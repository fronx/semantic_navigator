# Chunk-Based Ingestion Pipeline Plan

## Context

We are building a chunk-based ingestion pipeline that processes markdown files into semantic chunks, extracts and deduplicates keywords, and stores everything in Supabase for the TopicsView visualization.

The approach is **REPL-driven development** with **progressive cache layers**. Each step adds functionality and creates a cache file that lets us resume development at the next step without re-running expensive operations (LLM calls, embeddings, database operations).

## Current State

**What we have** (as of 2026-02-09):

- ✅ **Step 1: Chunking** - LLM-based semantic chunking with Haiku
  - Cache: `data/chunks-cache.json`
  - Format: `{ "file_path": [{ content, position, headingContext, chunkType, keywords }] }`
  - Chunker extracts initial keywords from each chunk

- ✅ **Step 2: Keyword Similarity** - Embedding-based similarity matrix
  - Cache: `data/top-similar.json`
  - Format: `{ "keyword": [["similar_keyword", score], ...] }`
  - Uses OpenAI text-embedding-3-small (1536 dimensions)

- ✅ **Step 3: Keyword Deduplication** - LLM-based merge decisions
  - Cache: `data/chunks-keywords-deduplicated.json`
  - Format: Same as chunks-cache but with canonical keywords applied
  - Haiku decides whether to merge similar keywords (e.g., "role" ← "roles")

- ✅ **Step 4: Keyword Preparation** - Final keyword records with embeddings
  - Cache: `data/keywords-prepared.json`
  - Format: `{ keywordRecords: [...], keywordOccurrences: [...] }`
  - Ready for database insertion

**Library functions** (extracted from REPL script):
- `src/lib/keyword-similarity.ts` - Generic similarity and clustering utilities
- `src/lib/keyword-deduplication.ts` - LLM-based deduplication logic

## Goal

Extend the REPL script to perform **complete chunk-based database ingestion**, creating:
- Article nodes (minimal containers with summaries)
- Chunk nodes (with content and embeddings)
- Keyword records (canonical, with embeddings)
- Keyword occurrences (linking keywords to chunks AND articles)
- Containment edges (article → chunks hierarchy)

This will enable the TopicsView to render chunk-based graphs with semantic zoom and keyword clustering.

## Why Articles Are Required

Even for chunk-based visualization, we must create article nodes because:
- **Backlinks**: Wiki-style `[[links]]` resolve to articles by filename
- **Project associations**: User-curated projects link to articles at the top level
- **Reimport handling**: System saves/restores project associations to articles
- **MapView**: Shows article-keyword bipartite graphs (separate from TopicsView)
- **Hierarchy**: `source_path` groups chunks conceptually, but `containment_edges` makes it queryable

**Key insight**: Articles don't store original file content (that's in chunks). They store LLM-generated summaries and embeddings for high-level search/navigation.

## Approach: Progressive Cache Layers

Each step:
1. **Checks cache first** - Skip if output already exists
2. **Performs operation** - LLM call, embedding generation, or database write
3. **Writes new cache** - Enables resuming at the next step
4. **Verification** - How to confirm this step worked

We can always re-run the script. Early steps return cached data instantly. Only new/uncached steps execute.

## Steps to Complete

### Step 5: Article Summary and Keyword Generation

**What**: Generate LLM summaries and extract keywords for each markdown file.

**Input**: Original file content (from vault)

**Operation**:
- Call `generateArticleSummary(title, content)` from `src/lib/summarization.ts`
- Call `extractKeywordsFromText(summary)` to get article-level keywords
- One Haiku call per file for summary (cached per file)
- Keywords extracted from summary, not from full content (chunks already have their keywords)

**Output Cache**: `data/article-summaries.json`
```json
{
  "file_path": {
    "title": "Article Title",
    "summary": "Two-sentence summary generated by Haiku",
    "keywords": ["keyword1", "keyword2", "keyword3"]
  }
}
```

**Verification**:
- Read cache file, check all files are present
- Spot-check summaries are 2-3 sentences, capture main themes
- Spot-check keywords are relevant to summaries

### Step 6: Content Embedding Generation

**What**: Generate embeddings for article summaries and chunk contents.

**Input**:
- Article summaries from Step 5
- Chunk contents from Step 1 (deduplicated keywords from Step 3)
- Unique keywords from Step 4

**Operation**:
- Batch all texts: `[summary1, summary2, ..., chunk1, chunk2, ..., keyword1, keyword2, ...]`
- Call `generateEmbeddingsBatched()` once
- Split results back into: article embeddings, chunk embeddings, keyword embeddings

**Output Cache**: `data/content-embeddings.json`
```
{
  "articles": {
    "file_path": [0.123, 0.456, ...],  // 1536-dim vector
  },
  "chunks": {
    "file_path": {
      "position": [0.123, 0.456, ...]  // 1536-dim vector per chunk
    }
  },
  "keywords": {
    "keyword": [0.123, 0.456, ...]  // 1536-dim vector (reuse from Step 4)
  }
}
```

**Verification**:
- Check all articles have embeddings (1536 floats each)
- Check all chunks have embeddings
- Count matches file/chunk counts from earlier steps

### Step 7: Content Hashing

**What**: Generate SHA256 hashes of original file content for change detection.

**Input**: Original file content (from vault)

**Operation**:
- Hash each file's full content: `createHash('sha256').update(content).digest('hex').slice(0, 16)`

**Output Cache**: `data/content-hashes.json`
```
{
  "file_path": "a1b2c3d4e5f6g7h8"  // 16-char hex hash
}
```

**Verification**:
- All files have hashes
- Re-running produces same hashes (deterministic)
- Changing a file produces different hash

### Step 8: Database Insertion (Dry Run)

**What**: Prepare and validate database insertion payloads without actually writing to DB.

**Input**:
- Article summaries (Step 5)
- Content embeddings (Step 6)
- Content hashes (Step 7)
- Deduplicated chunks (Step 3)
- Keyword records (Step 4)

**Operation**:
- Build complete database payloads:
  - **Article nodes**: `{ title, summary, embedding, source_path, content_hash, node_type: "article" }`
  - **Chunk nodes**: `{ content, embedding, source_path, content_hash, node_type: "chunk", chunk_type, heading_context }`
  - **Keywords**: `{ keyword, embedding, embedding_256 }` (embedding_256 = first 256 dims)
  - **Article keywords**: Extract from article summaries (for MapView)
  - **Chunk keywords**: Already extracted during chunking (Step 1)
  - **Keyword occurrences**: `{ keyword_text, node_ref, node_type }` where node_ref is `{source_path, position?}`
  - **Containment edges**: `{ parent_source_path, child_position, position }` (positions for ordering)
- Validate all required fields are present
- Check structural integrity (every chunk has parent article, every occurrence has valid keyword)

**Output Cache**: `data/db-payloads.json`
```json
{
  "articles": [{
    "title": "Article Name",
    "summary": "Two-sentence summary",
    "embedding": [0.123, ...],  // 1536 floats
    "source_path": "vault/article.md",
    "content_hash": "a1b2c3d4e5f6g7h8",
    "node_type": "article"
  }],
  "chunks": [{
    "content": "Chunk text...",
    "embedding": [0.456, ...],  // 1536 floats
    "source_path": "vault/article.md",
    "content_hash": "a1b2c3d4e5f6g7h8",  // Same as parent article
    "node_type": "chunk",
    "chunk_type": "definition",
    "heading_context": ["Introduction", "Background"],
    "position": 0  // Position within file
  }],
  "keywords": [{
    "keyword": "canonical keyword",
    "embedding": [0.789, ...],  // 1536 floats
    "embedding_256": [0.789, ...]  // First 256 floats
  }],
  "articleKeywords": {
    "vault/article.md": ["keyword1", "keyword2"]
  },
  "chunkKeywords": {
    "vault/article.md": {
      "0": ["keyword1", "keyword3"],  // Position → keywords
      "1": ["keyword2", "keyword4"]
    }
  },
  "containmentEdges": [{
    "parent_source_path": "vault/article.md",
    "child_position": 0,
    "position": 0
  }]
}
```

**Verification**:
- All payloads have required fields (no nulls where not allowed)
- Counts match: N articles, M total chunks, P unique keywords
- All chunks have parent articles (via source_path)
- All keyword occurrences reference valid keywords and nodes
- Article keywords extracted and present
- Embeddings are valid arrays of correct length (1536 and 256)

### Step 9: Database Insertion (Live)

**What**: Execute database writes to Supabase with idempotent change detection.

**Input**: Validated payloads from Step 8

**Operation** (per file, in order):

1. **Check existing article**:
   ```sql
   SELECT id, content_hash FROM nodes
   WHERE source_path = $source_path AND node_type = 'article'
   ```

2. **If exists and hash matches**: Skip (no changes)

3. **If exists and hash differs**:
   - Delete article node (cascades to chunks, edges, occurrences via FK constraints)
   - Proceed to insert new article + chunks

4. **If not exists**: Proceed to insert

5. **Insert article node**:
   ```sql
   INSERT INTO nodes (title, summary, embedding, source_path, content_hash, node_type)
   VALUES ($title, $summary, $embedding, $source_path, $content_hash, 'article')
   RETURNING id
   ```

6. **Insert chunk nodes** (batch per article):
   ```sql
   INSERT INTO nodes (content, embedding, source_path, content_hash, node_type, chunk_type, heading_context)
   VALUES
     ($content1, $embedding1, $source_path, $content_hash, 'chunk', $chunk_type1, $heading_context1),
     ($content2, $embedding2, $source_path, $content_hash, 'chunk', $chunk_type2, $heading_context2),
     ...
   RETURNING id
   ```

7. **Upsert keywords** (batch all unique keywords):
   ```sql
   INSERT INTO keywords (keyword, embedding, embedding_256)
   VALUES
     ($keyword1, $embedding1, $embedding_256_1),
     ($keyword2, $embedding2, $embedding_256_2),
     ...
   ON CONFLICT (keyword) DO UPDATE SET
     embedding = EXCLUDED.embedding,
     embedding_256 = EXCLUDED.embedding_256
   RETURNING id, keyword
   ```

8. **Insert containment edges** (batch per article):
   ```sql
   INSERT INTO containment_edges (parent_id, child_id, position)
   VALUES
     ($article_id, $chunk_id_1, 0),
     ($article_id, $chunk_id_2, 1),
     ...
   ```

9. **Insert keyword occurrences** (batch per article):
   ```sql
   INSERT INTO keyword_occurrences (keyword_id, node_id, node_type)
   VALUES
     -- Article keywords
     ($keyword_id_1, $article_id, 'article'),
     ($keyword_id_2, $article_id, 'article'),
     -- Chunk keywords
     ($keyword_id_3, $chunk_id_1, 'chunk'),
     ($keyword_id_4, $chunk_id_1, 'chunk'),
     ($keyword_id_5, $chunk_id_2, 'chunk'),
     ...
   ON CONFLICT (keyword_id, node_id) DO NOTHING
   ```

**Output**: No cache file (operation is idempotent via hash-based change detection)

**Verification Queries**:

```sql
-- Count articles and chunks
SELECT node_type, COUNT(*) FROM nodes GROUP BY node_type;

-- Count unique keywords
SELECT COUNT(*) FROM keywords;

-- Count keyword occurrences by type
SELECT node_type, COUNT(*) FROM keyword_occurrences GROUP BY node_type;

-- Verify all chunks have parent articles
SELECT COUNT(*) FROM nodes n
WHERE n.node_type = 'chunk'
  AND NOT EXISTS (
    SELECT 1 FROM containment_edges ce
    WHERE ce.child_id = n.id
  );
-- Should return 0

-- Spot-check: Get article with chunks
SELECT n.id, n.title, n.summary,
  (SELECT COUNT(*) FROM containment_edges WHERE parent_id = n.id) as chunk_count
FROM nodes n
WHERE n.source_path = 'vault/example.md' AND n.node_type = 'article';

-- Spot-check: Get keywords for a chunk
SELECT k.keyword
FROM keywords k
JOIN keyword_occurrences ko ON ko.keyword_id = k.id
WHERE ko.node_id = $chunk_id AND ko.node_type = 'chunk';

-- Verify containment edge integrity
SELECT
  (SELECT COUNT(*) FROM containment_edges) as total_edges,
  (SELECT COUNT(DISTINCT parent_id) FROM containment_edges) as distinct_parents,
  (SELECT COUNT(DISTINCT child_id) FROM containment_edges) as distinct_children;
```

### Step 10: Backlinks (Future)

**What**: Extract and insert wiki-style backlinks between articles.

**Input**: Original markdown content (parse `[[link]]` syntax)

**Operation**:
- Parse backlinks from markdown using `parseMarkdown()` from `src/lib/parser.ts`
- Resolve link text to target article by filename matching `source_path`
- Insert into `backlink_edges` table

**Status**: Deferred until article-level features are needed. Chunks don't participate in backlinks.

## Success Criteria

The pipeline is complete when:

1. **Data completeness**: All markdown files from vault are processed and stored
2. **TopicsView renders**: Chunk-based graph displays with keyword nodes and chunk nodes
3. **Search works**: Semantic search finds relevant chunks by keyword or content
4. **Idempotency**: Re-running ingestion on unchanged files is fast (skips via hash check)
5. **Reimport works**: Changing a file and re-ingesting updates DB correctly
6. **Keywords deduplicated**: Singular/plural variations merged (verified by spot-checking DB)

## REPL Development Pattern

The script follows this pattern:

```
// Step 1: Load or generate chunks
let chunksMap = await getOrGenerateChunks(vaultFiles, cacheFile)

// Step 2: Load or generate similarity
let topSimilar = await getOrGenerateTopSimilar(chunksMap, cacheFile)

// Step 3: Load or apply deduplication
let deduplicatedChunks = await getOrApplyDeduplication(chunksMap, topSimilar, cacheFile)

// Step 4: Load or prepare keywords
let keywordData = await getOrPrepareKeywords(deduplicatedChunks, cacheFile)

// NEW Step 5: Load or generate article summaries
let articleSummaries = await getOrGenerateArticleSummaries(vaultFiles, cacheFile)

// NEW Step 6: Load or generate content embeddings
let contentEmbeddings = await getOrGenerateContentEmbeddings(articleSummaries, deduplicatedChunks, keywordData, cacheFile)

// NEW Step 7: Load or generate content hashes
let contentHashes = await getOrGenerateContentHashes(vaultFiles, cacheFile)

// NEW Step 8: Dry run database insertion
let dbPayloads = await prepareDatabasePayloads(articleSummaries, contentEmbeddings, contentHashes, deduplicatedChunks, keywordData, cacheFile)

// NEW Step 9: Live database insertion
await insertToDatabase(dbPayloads, { dryRun: false })
```

Each `getOrGenerate*` function:
1. Checks if cache file exists and is valid
2. If yes: loads and returns cached data
3. If no: performs operation, writes cache, returns data

This lets us:
- Iterate rapidly on later steps without re-running expensive early steps
- Debug issues at any layer by inspecting cache files
- Restart from any point by deleting cache files for that step and beyond

## Database Schema

The Supabase schema supports canonical keywords with many-to-many relationships:

### Tables

**keywords** (canonical, one row per unique keyword):
- `id` UUID (PK)
- `keyword` TEXT (UNIQUE)
- `embedding` VECTOR(1536)
- `embedding_256` VECTOR(256) - First 256 dimensions of embedding for bandwidth optimization
- `created_at` TIMESTAMPTZ

**keyword_occurrences** (many-to-many join):
- `keyword_id` UUID (FK to keywords)
- `node_id` UUID (FK to nodes)
- `node_type` TEXT ('article' or 'chunk') - Denormalized for efficient filtering
- `created_at` TIMESTAMPTZ
- PK: (keyword_id, node_id)

**nodes** (articles, chunks, projects):
- `id` UUID (PK)
- `node_type` TEXT ('article', 'chunk', 'project')
- `content` TEXT (nullable) - Populated for chunks only
- `summary` TEXT (nullable) - Populated for articles only
- `content_hash` TEXT - For change detection
- `embedding` VECTOR(1536) (nullable)
- `source_path` TEXT (nullable) - File path relative to vault
- `title` TEXT (nullable) - For articles
- `chunk_type` TEXT (nullable) - For chunks (e.g., "problem statement")
- `heading_context` TEXT[] (nullable) - For chunks (breadcrumb array)
- Other fields...

**containment_edges** (hierarchy):
- `id` UUID (PK)
- `parent_id` UUID (FK to nodes)
- `child_id` UUID (FK to nodes)
- `position` INTEGER - Order of child within parent
- `created_at` TIMESTAMPTZ

### Data Model for Chunk-Based Ingestion

**Article Node** (one per file):
- `node_type` = 'article'
- `title` = filename (without .md extension)
- `summary` = LLM-generated summary
- `embedding` = embedding of summary
- `source_path` = relative file path
- `content_hash` = SHA256 of original file content (first 16 chars)
- `content` = NULL (articles don't store original content)

**Chunk Node** (many per file):
- `node_type` = 'chunk'
- `content` = actual chunk text
- `embedding` = embedding of chunk content
- `source_path` = same as parent article
- `content_hash` = same as parent article (chunks are derived from article)
- `chunk_type` = semantic type from LLM (e.g., "definition", "example")
- `heading_context` = array of heading breadcrumbs (e.g., ["Introduction", "Background"])
- `summary` = NULL (chunks don't have summaries)

**Keyword Record** (canonical):
- `keyword` = canonical form (e.g., "role" not "roles")
- `embedding` = embedding of keyword text
- `embedding_256` = first 256 dimensions of embedding

**Keyword Occurrences**:
- Link keywords to chunks: (keyword_id, chunk.id, 'chunk')
- Link keywords to articles: (keyword_id, article.id, 'article')
- Article keywords extracted from summaries, chunk keywords from chunk content

**Containment Edges**:
- Link article to chunks: (article.id, chunk.id, position)
- Position indicates chunk order within file

### Insertion Strategy

1. **Check if article exists** by `source_path`
2. **If exists**: Compare `content_hash`
   - If hash matches: Skip (no changes)
   - If hash differs: Delete article + all descendants (chunks, edges, occurrences), then recreate
3. **If not exists**: Create new
4. **Insert article node**, capture UUID
5. **Insert chunk nodes** (batch), capture UUIDs
6. **Upsert keywords** (canonical table, `ON CONFLICT (keyword) DO UPDATE`)
7. **Insert containment edges** (article → chunks)
8. **Upsert keyword occurrences** (link keywords to articles and chunks)

## Next Actions

1. ✅ Review database schema and insertion strategy
2. Start implementing Step 5 (article summaries) in the REPL script
3. Extract reusable functions to library files as they stabilize
4. Test each step's cache/resume behavior
5. Continue through Steps 6-9 sequentially
6. Write verification queries for Step 9 (database checks)

## Notes

- **Composability**: Functions should be pure where possible, accepting inputs and returning outputs rather than mutating global state
- **Functional style**: Prefer `map`, `filter`, `reduce` over imperative loops where it improves readability
- **Error handling**: Let errors bubble up with full stack traces; only catch when we have a specific recovery strategy
- **Performance**: Batching is critical for embeddings (OpenAI has rate limits); batch 100 texts at a time
- **Library extraction**: When a function becomes stable and reusable, extract to `src/lib/` and import in REPL
